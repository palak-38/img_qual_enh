{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3342171,"sourceType":"datasetVersion","datasetId":2017696},{"sourceId":4853613,"sourceType":"datasetVersion","datasetId":2813430}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nimport random\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\ndef create_lr_image(hr_image, scale_factor=2, degradation='bicubic'):\n    h, w, c = hr_image.shape\n    new_h, new_w = h // scale_factor, w // scale_factor\n    if degradation == 'bicubic':\n        lr_image = cv2.resize(hr_image, (new_w, new_h), interpolation=cv2.INTER_CUBIC)\n    else:\n        lr_image = cv2.resize(hr_image, (new_w, new_h), interpolation=cv2.INTER_AREA)\n    return lr_image\ndef img_to_tensor(img):\n    img = np.transpose(img, (2, 0, 1))\n    img = img.astype(np.float32) / 255.0\n    return torch.from_numpy(img)\nclass SuperResolutionDataset(Dataset):\n    def __init__(self, \n                 hr_dir, \n                 patch_size=128, \n                 scale_factor=2, \n                 transform=None):\n        self.hr_dir = hr_dir\n        self.patch_size = patch_size\n        self.scale_factor = scale_factor\n        self.transform = transform\n        self.image_files = [os.path.join(hr_dir, f) \n                            for f in os.listdir(hr_dir) if f.lower().endswith(('png','jpg','jpeg'))]\n    def __len__(self):\n        return len(self.image_files)\n    def __getitem__(self, idx):\n        hr_path = self.image_files[idx]\n        hr_bgr = cv2.imread(hr_path)  # BGR format\n        hr_rgb = cv2.cvtColor(hr_bgr, cv2.COLOR_BGR2RGB)\n        h, w, c = hr_rgb.shape\n        if h < self.patch_size or w < self.patch_size:\n            hr_rgb = cv2.resize(hr_rgb, (self.patch_size, self.patch_size), interpolation=cv2.INTER_CUBIC)\n            h, w, c = hr_rgb.shape\n        top = random.randint(0, h - self.patch_size)\n        left = random.randint(0, w - self.patch_size)\n        hr_patch = hr_rgb[top:top+self.patch_size, left:left+self.patch_size, :]\n        lr_patch = create_lr_image(hr_patch, scale_factor=self.scale_factor)\n        if self.transform:\n            hr_patch, lr_patch = self.transform(hr_patch, lr_patch)\n        hr_tensor = img_to_tensor(hr_patch)  \n        lr_tensor = img_to_tensor(lr_patch)\n        return lr_tensor, hr_tensor","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-15T16:21:11.139344Z","iopub.execute_input":"2025-01-15T16:21:11.139676Z","iopub.status.idle":"2025-01-15T16:21:14.496692Z","shell.execute_reply.started":"2025-01-15T16:21:11.139648Z","shell.execute_reply":"2025-01-15T16:21:14.496020Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class SimpleSRNet(nn.Module):\n    def __init__(self, scale_factor=2, num_channels=3, base_channels=64):\n        super(SimpleSRNet, self).__init__()\n        self.scale_factor = scale_factor\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(num_channels, base_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(base_channels, base_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n        )\n        self.conv2 = nn.Conv2d(base_channels, base_channels * (scale_factor**2), kernel_size=3, padding=1)\n        self.pixel_shuffle = nn.PixelShuffle(scale_factor)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv3 = nn.Conv2d(base_channels, num_channels, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        feat = self.conv1(x)\n        upsampled = self.conv2(feat)       \n        upsampled = self.pixel_shuffle(upsampled)  \n        upsampled = self.relu2(upsampled)\n        out = self.conv3(upsampled)        \n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T16:21:14.497786Z","iopub.execute_input":"2025-01-15T16:21:14.498209Z","iopub.status.idle":"2025-01-15T16:21:14.504283Z","shell.execute_reply.started":"2025-01-15T16:21:14.498176Z","shell.execute_reply":"2025-01-15T16:21:14.503599Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import argparse\nimport sys\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom torch import nn, optim\n\n# Assuming SuperResolutionDataset and SimpleSRNet are defined elsewhere.\n\ndef train_super_resolution():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--hr_dir', \n        type=str, \n        default='/kaggle/input/div2k-high-resolution-images/DIV2K_train_HR/DIV2K_train_HR', \n        help='Path to high-resolution training images'\n    )\n    parser.add_argument(\n        '--val_dir', \n        type=str, \n        default='/kaggle/input/div2k-high-resolution-images/DIV2K_valid_HR/DIV2K_valid_HR', \n        help='Path to high-resolution validation images'\n    )\n    parser.add_argument('--patch_size', type=int, default=128, help='Training patch size')\n    parser.add_argument('--scale_factor', type=int, default=4, help='SR scale factor')\n    parser.add_argument('--lr', type=float, default=1e-4, help='Learning rate')\n    parser.add_argument('--epochs', type=int, default=10, help='Number of training epochs')\n    parser.add_argument('--batch_size', type=int, default=8, help='Batch size')\n    \n    # Filter out unwanted Jupyter arguments\n    args, unknown = parser.parse_known_args()\n    \n    # Create dataset\n    train_dataset = SuperResolutionDataset(\n        hr_dir=args.hr_dir,\n        patch_size=args.patch_size,\n        scale_factor=args.scale_factor,\n    )\n    val_dataset = SuperResolutionDataset(\n        hr_dir=args.val_dir,\n        patch_size=args.patch_size,\n        scale_factor=args.scale_factor,\n    )\n    \n    # DataLoaders\n    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=2)\n    \n    # Initialize model\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = SimpleSRNet(scale_factor=args.scale_factor)\n    model.to(device)\n    \n    # Loss and optimizer\n    criterion = nn.MSELoss()  \n    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n    \n    # Training Loop\n    for epoch in range(args.epochs):\n        model.train()\n        train_loss = 0.0\n        for lr_patches, hr_patches in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{args.epochs}\"):\n            lr_patches, hr_patches = lr_patches.to(device), hr_patches.to(device)\n            \n            # Forward\n            sr_patches = model(lr_patches)\n            loss = criterion(sr_patches, hr_patches)\n            \n            # Backward\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n        train_loss /= len(train_loader)\n        \n        # Validation\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for lr_patches, hr_patches in val_loader:\n                lr_patches, hr_patches = lr_patches.to(device), hr_patches.to(device)\n                sr_patches = model(lr_patches)\n                loss = criterion(sr_patches, hr_patches)\n                val_loss += loss.item()\n        val_loss /= len(val_loader)\n        \n        print(f\"Epoch [{epoch+1}/{args.epochs}] - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n    \n    # Save model\n    torch.save(model.state_dict(), \"sr_model.pth\")\n    print(\"Model saved to sr_model.pth\")\n\n\nif __name__ == \"__main__\":\n    # Filter sys.argv to ignore Jupyter/Colab's unwanted arguments\n    sys.argv = [sys.argv[0]]\n    train_super_resolution()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T16:21:14.505838Z","iopub.execute_input":"2025-01-15T16:21:14.506032Z","iopub.status.idle":"2025-01-15T16:29:03.476604Z","shell.execute_reply.started":"2025-01-15T16:21:14.506016Z","shell.execute_reply":"2025-01-15T16:29:03.475479Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 100/100 [00:47<00:00,  2.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10] - Train Loss: 0.0617, Val Loss: 0.0190\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 100/100 [00:41<00:00,  2.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/10] - Train Loss: 0.0122, Val Loss: 0.0096\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 100/100 [00:40<00:00,  2.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/10] - Train Loss: 0.0074, Val Loss: 0.0073\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 100/100 [00:40<00:00,  2.47it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/10] - Train Loss: 0.0059, Val Loss: 0.0060\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 100/100 [00:40<00:00,  2.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/10] - Train Loss: 0.0055, Val Loss: 0.0052\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 100/100 [00:39<00:00,  2.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/10] - Train Loss: 0.0052, Val Loss: 0.0055\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 100/100 [00:41<00:00,  2.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/10] - Train Loss: 0.0051, Val Loss: 0.0051\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 100/100 [00:40<00:00,  2.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/10] - Train Loss: 0.0049, Val Loss: 0.0056\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 100/100 [00:40<00:00,  2.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/10] - Train Loss: 0.0047, Val Loss: 0.0054\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 100/100 [00:39<00:00,  2.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/10] - Train Loss: 0.0048, Val Loss: 0.0050\nModel saved to sr_model.pth\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def stitch_patches(patches, patch_coords, out_h, out_w):\n    stitched_image = np.zeros((out_h, out_w, 3), dtype=np.float32)\n    weight_map = np.zeros((out_h, out_w, 3), dtype=np.float32)\n    \n    for (patch, (top, left)) in zip(patches, patch_coords):\n        ph, pw, _ = patch.shape\n        stitched_image[top:top+ph, left:left+pw, :] += patch\n        weight_map[top:top+ph, left:left+pw, :] += 1.0\n    stitched_image /= np.maximum(weight_map, 1e-8)\n    return stitched_image\n\ndef inference_large_image(model_path, lr_image_path, scale_factor=2, patch_size=128, overlap=16):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    model = SimpleSRNet(scale_factor=scale_factor)\n    model.load_state_dict(torch.load(model_path, map_location=device))\n    model.to(device)\n    model.eval()\n\n    lr_bgr = cv2.imread(lr_image_path)\n    lr_rgb = cv2.cvtColor(lr_bgr, cv2.COLOR_BGR2RGB)\n    lr_h, lr_w, _ = lr_rgb.shape\n\n    hr_h, hr_w = lr_h * scale_factor, lr_w * scale_factor\n\n    patches = []\n    patch_coords = []\n    step = patch_size - overlap\n\n    for top in range(0, lr_h, step):\n        for left in range(0, lr_w, step):\n            bottom = min(top + patch_size, lr_h)\n            right = min(left + patch_size, lr_w)\n\n            lr_patch = lr_rgb[top:bottom, left:right, :]\n            lr_patch_t = img_to_tensor(lr_patch).unsqueeze(0).to(device)  \n            with torch.no_grad():\n                sr_patch_t = model(lr_patch_t)  \n\n            sr_patch = sr_patch_t.squeeze(0).cpu().numpy().transpose(1,2,0)  # [H*scale, W*scale, 3]\n            sr_patch = np.clip(sr_patch, 0.0, 1.0)\n\n            patches.append(sr_patch)\n            patch_coords.append((top*scale_factor, left*scale_factor))\n\n    sr_image = stitch_patches(patches, patch_coords, hr_h, hr_w)\n\n    sr_image_8u = (sr_image * 255.0).astype(np.uint8)\n\n    result_bgr = cv2.cvtColor(sr_image_8u, cv2.COLOR_RGB2BGR)\n    out_path = 'sr_result.png'\n    cv2.imwrite(out_path, result_bgr)\n    print(f\"Super-resolved image saved to {out_path}\")\n\n\nif __name__ == \"__main__\":\n    inference_large_image(\n        model_path=\"sr_model.pth\", \n        lr_image_path=\"/kaggle/input/flickr2k/Flickr2K/Flickr2K_LR_bicubic/X2/000001x2.png\",\n        scale_factor=4,\n        patch_size=64,\n        overlap=8\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T16:29:03.478197Z","iopub.execute_input":"2025-01-15T16:29:03.478960Z","iopub.status.idle":"2025-01-15T16:29:05.785794Z","shell.execute_reply.started":"2025-01-15T16:29:03.478927Z","shell.execute_reply":"2025-01-15T16:29:05.784930Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-4-ad967b871c9c>:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(model_path, map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"Super-resolved image saved to sr_result.png\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}